# CNN图像分类模型与GradCAM可解释性分析
选这个题目的灵感来自我本学期的另一节课程人工智慧实务，老师在课程中提到explainable ai。本研究由chatgpt协助完成与分析。

## 研究動機：解開 AI 模型分類的「黑箱」
在現代人工智慧的實務應用中，卷積神經網路 (CNN) 已經成為處理圖像分類任務的標準技術。其在圖像識別、物件偵測等領域展現出卓越的效能，廣泛應用於醫療影像分析、自動駕駛等關鍵領域。然而，CNN 模型內部運作如同一個「黑箱」，我們很難直接理解模型為何做出特定的分類決策。這種不透明性限制了 AI 在高風險領域的應用，也使得開發者難以偵錯或改進模型表現。為了解決這個問題並提升 AI 模型的信任度，同時觀察模型在分類過程中可能存在的錯誤源頭，本項目將深入運用 GradCAM 技術。透過視覺化模型關注區域，我們希望能為「黑箱」模型打開一扇窗。

## 目標與方法
### 1.建構 CNN 模型
我們將使用強大的 TensorFlow 框架，從零開始建構一個卷積神經網路（CNN）模型。
這個模型將針對圖像分類任務進行優化，以實現高精度的分類效能。
### 2.訓練與資料集
模型將使用廣泛認可的 CIFAR-10 圖像資料集以及AFHQ资料集分别進行訓練、互相验证准确性，并使用我自己在网上寻找的图片验证三个模型的可用性。<br>
正规化的部分，我将像素值標準化至 0 到 1 之間，并统一 resize 为 32×32 大小；AFHQ在正规化之前进行了类别过滤，仅保留cat和dog两个类别，用于二分类任务。

## 模型架构设计
对于CIFAR-10，本研究所建構的模型基於 Keras 所提供的高階 API，採用 Sequential 架構堆疊三層卷積區塊與兩層全連接層。輸入維度為 32x32x3，輸出為 10 個類別的機率分佈。模型以 Adam 作為優化器，損失函數為 sparse_categorical_crossentropy，評估指標為準確率（accuracy）。对于AFHQ，输出层調整為 Dense(1, sigmoid)，實現二分類架構，並使用 binary_crossentropy 為損失函數。

## 训练与测试过程
### CIFAR-10 
模型訓練過程使用 CIFAR-10 訓練資料集進行 10 個 epoch，batch size 設為 64，並使用 Adam 優化器進行權重更新。訓練過程中，模型逐步收斂，顯示其具備穩定的學習能力。
在每個 epoch 結束後，皆在測試集上進行驗證，其結果顯示，訓練集準確率從第一輪的約 32% 持續上升至最終約 76.5%；驗證集準確率則穩定維持於約 70% 左右。整體而言，本模型可在少量參數下有效學習分類任務，顯示卷積網路對於影像特徵萃取具備高度效能。
### AFHQ
在 AFHQ 任務中，選取其中的 cat 與 dog 子類進行訓練。訓練集與測試集比例約為 8:2，模型使用與 CIFAR-10 相同的架構與訓練參數（但輸出改為 sigmoid），經過 10 個 epoch 後，訓練準確率達 0.83，驗證準確率穩定於 0.73 左右，顯示模型對於真實高品質資料亦具備良好擬合能力。

## 模型效能评估
### CIFAR-10
模型在測試集上的總體準確率為 70.8%，符合預期。進一步透過混淆矩陣與分類報告進行細項分析，可以觀察到模型在部分類別的辨識效果明顯較佳，例如：<br>
automobile 類別的 precision 達 0.86，recall 為 0.79，顯示其容易被正確識別；<br>
cat 類別表現則相對較差，precision 與 recall 僅約 0.50~0.56，顯示易與其他動物類別混淆，尤其是 dog。<br>
分類報告（classification report）則進一步提供每一類別的 precision、recall 與 F1-score。綜合分析可見，本模型在具結構性特徵的類別（如 automobile、ship）上表現優異，說明其在提取邊緣與幾何形狀等特徵方面具備良好能力；但在動物類別（如 cat、dog）則相對困難，推測與影像模糊、姿勢多變及語意模糊性有關。
<br>此外，混淆矩陣亦揭示了 cat 與 dog 間存在明顯誤判傾向，顯示模型在區分語意相近的物種時仍有挑戰。未來若欲提升辨識精度，可考慮導入更深層模型或進行類別特化強化訓練。

### AFHQ
為驗證模型在真實世界資料上的辨識能力，本研究將先前於 CIFAR-10 上訓練完成的 CNN 模型，應用於 AFHQ 動物臉部資料集進行二分類測試（cat vs dog）。測試結果顯示，模型在 AFHQ 測試集上取得 98% 的總體準確率，顯著高於在 CIFAR-10 測試集上的結果，展現出極佳的遷移與泛化能力。<br>
分類報告中，cat 類別的 precision 為 0.98、recall 為 0.99，dog 類別則為 0.99 與 0.98，F1-score 同樣達 0.98，顯示模型對兩類的辨識表現皆相當穩定且準確。這也意味著模型不僅能從 CIFAR-10 中學習到一般化的動物特徵，亦能有效應用於來自網路、背景多樣、品種豐富的高解析動物圖像。<br>
此外，透過 GradCAM 可視化技術進一步檢視模型的注意焦點，可見其對於 cat 類別主要聚焦於臉部與耳部區域，而對 dog 類別則偏重於鼻口與頭部輪廓，這些結果與人類視覺判斷依據高度一致，進一步強化模型解釋性的可信度。
整體而言，模型在 AFHQ 測試資料上展現出色的分類能力與語意關注能力，驗證其已不僅侷限於原訓練資料集，具備良好的應用潛力與擴展性。

## 交叉测试
為評估模型在不同資料來源下的泛化能力，本研究進行了兩項交叉測試實驗。
### CIFAR-10 模型測試 AFHQ 資料
以 CIFAR-10 訓練的模型應用於 AFHQ 的 cat 與 dog 類別，共 12,000 張影像，模型整體分類準確率為 58%，表現顯著低於在原始測試集上的結果。混淆矩陣與分類報告顯示：<br>
「cat」precision 為 0.59，recall 為 0.52，f1-score 為 0.55。<br>
「dog」precision 為 0.57，recall 為 0.64，f1-score 為 0.60。<br>
模型傾向於混淆兩類，尤其將部分 dog 錯判為 cat。<br>
這反映出 CIFAR-10 模型在面對解析度更高、背景複雜度更大的 AFHQ 資料時，其泛化能力受到限制。推測原因為 CIFAR-10 資料較模糊、視覺特徵受限，導致模型所學特徵難以應用於更寫實的圖像中。
### AFHQ 模型測試 CIFAR-10 資料
以 AFHQ 資料訓練的二分類模型應用於 CIFAR-10 中的 cat 與 dog 類別（共 10,727 張圖），模型整體準確率降至 38%，分類報告如下：<br>
cat 類別 recall 僅為 0.39，dog recall 亦僅 0.36。
混淆矩陣顯示模型不僅容易將 cat 判為其他類別（如 dog、frog、truck），亦將 dog 判為 cat 或其他動物。<br>
AFHQ 模型在此場景表現更為嚴峻，主因為其訓練時所見圖像大多為居中構圖、高品質清晰臉部，缺乏處理 CIFAR-10 中姿態隨機、背景雜亂、低解析度圖像的能力。因此，模型對 CIFAR-10 的適應性有限，亦反映出資料來源差異對深度學習模型泛化能力的顯著影響。

## 可解释性分析
儘管卷積神經網路（CNN）在圖像分類中表現優異，但其內部決策邏輯仍屬於難以直接解釋的「黑盒模型」。為提升模型的可解釋性與使用者信任度，本研究採用 GradCAM（Gradient-weighted Class Activation Mapping）技術，將模型在判斷過程中「關注」的圖像區域可視化，觀察其是否聚焦於語意合理的區域。<br>
GradCAM 的原理為：透過對最後一層卷積特徵圖（feature map）與分類輸出之間的梯度進行加權平均，生成一張與輸入圖像同尺寸的熱力圖，進而疊加於原圖上顯示模型「看的地方」。<br>
在實驗中，我們選擇了 cat 與 dog 兩類測試圖像，並繪製其 GradCAM 熱力圖。<br>對於「貓」類別，模型主要聚焦於臉部與耳朵位置，這些特徵對於辨識貓具有高度語意代表性；<br>
對於「狗」類別，模型關注區域多集中在頭部與身體中心，顯示模型具備對動物整體輪廓的識別能力。<br>
這些結果顯示，模型不僅能夠從像素層面學習特徵，也已捕捉到與人類相似的判斷依據。相比單純的準確率，GradCAM 提供了更高層次的「模型透明度」，特別有助於模型在實務應用中進行診斷、調整或對外解釋。

## 总结
本研究成功構建兩個基於卷積神經網路（CNN）的圖像分類模型，分別訓練於 CIFAR-10 與 AFHQ 資料集，並進行效能評估與跨資料集泛化測試，整體實驗成果顯著。<br>
在訓練階段，兩組模型皆展現良好的收斂性，CIFAR-10 模型最終於測試集上達成約 71% 的準確率，而 AFHQ 模型則達成高達 98% 的分類準確率，顯示在動物臉部特徵辨識上具備極佳表現。<br>
透過分類報告與混淆矩陣進行進一步分析，我們觀察到模型在結構明顯的類別（如 automobile、truck）上表現穩定，但在語意相近、視覺特徵重疊的類別（如 cat 與 dog）仍有明顯混淆。尤其 AFHQ 模型在 CIFAR 圖像上表現出極高 precision，但 recall 較低，反映出其特徵辨識策略更偏向「精確但嚴格」，相對對低解析資料的容忍度較低。相對地，CIFAR 模型則能模糊識別 AFHQ 動物，但較難掌握其細緻結構，導致整體準確率偏低（約 58%）。<br>
為進一步理解模型決策機制，本研究導入 GradCAM（Gradient-weighted Class Activation Mapping）技術進行可解釋性分析。結果顯示，模型能聚焦於語意關鍵區域（如動物臉部、耳朵、輪廓等），且對錯誤分類樣本亦展現出部分錯誤聚焦，提供了寶貴的診斷線索與優化依據。
